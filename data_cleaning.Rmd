---
title: "data_cleaning"
output: html_document
date: "2025-11-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Cleaning File 

This .Rmd file contains the data cleaning necessary for our project.

### Library Imports
```{r}
library(tidyverse)
# For language input
library(ellmer)
library(fastText)
library(cld2)
```


### Data Imports

The data, from the X website is updated daily. We take one day's worth of data

The notes can be found at the https://communitynotes.x.com/guide/en/under-the-hood/download-data, 
however it should be noted that only users with X accounts can download the data. 

A good description can be found at the same website.

As an overview the data takes on the following form: 
- `notes` contains information about a note generally and how it can be 

```{r}
# contains the notes and their current status
notes_uncleaned <- read_tsv("data/notes-00000.tsv")

# contains a rating by a specific user on a specific note
ratings_uncleaned <- read_tsv("data/ratings-00000.tsv")

# contains
notes_status_history_uncleaned <- read_tsv("data/noteStatusHistory-00000.tsv")

# Contains data for individual raters
# newUser: newly admitted users, who only have rating ability. 
# earnedIn: users who've earned writing ability.
# atRisk: users who are one Not Helpful note away from having writing ability locked. 
# earnedOutNoAcknowledge: users with writing ability locked that have not yet clicked 
# the acknowledgement button it in the product. 
# earnedOutAcknowledge: users who've lost the ability to write and acknowledged 
# it in the product, at which point their ratings start counting towards going back to earnedIn.
user_enrollment_status_uncleaned <- read_tsv("data/userEnrollment-00000.tsv")

# Note request data - contains a tweet, and a user who requested a 
# community note for a specific tweet
batSignals_cleaned <- read_tsv("data/batSignals-00000.tsv")

# Looking for bad notes 
# Looking for harmful notes
```


# Cleaning 
```{r}
# Cleaning to make time values interpretable 
notes <- notes_uncleaned %>% mutate(
  note_text = summary,
  time_created = as.POSIXct(createdAtMillis/1000, origin = "1970-01-01", tz = "UTC")) %>%
  dplyr::select(-summary, -createdAtMillis)

ratings <- ratings_uncleaned %>% mutate(
  time_created = as.POSIXct(createdAtMillis/1000, origin = "1970-01-01", tz = "UTC")) %>% 
  dplyr::select(-createdAtMillis)

notes_status_history <- notes_status_history_uncleaned %>% mutate(
  time_created = as.POSIXct(createdAtMillis/1000, origin = "1970-01-01", tz = "UTC"),
  timestamp_final_scoring_output = as.POSIXct(
    timestampMinuteOfFinalScoringOutput/1000, origin = "1970-01-01", tz = "UTC"), 
  timestamp_most_recent_change =  as.POSIXct(
    timestampMillisOfMostRecentStatusChange/1000, origin = "1970-01-01", tz = "UTC")
  ) %>% 
  dplyr::select(-createdAtMillis, -timestampMinuteOfFinalScoringOutput, -timestampMillisOfMostRecentStatusChange)

# batSignals - we can't ge the actual tweet information but here we can look at how well the 
# response is to a request (based on timestamp)
batSignals <- batSignals_cleaned %>% mutate(
  time_created = as.POSIXct(createdAtMillis/1000, origin = "1970-01-01", tz = "UTC")) %>% 
  dplyr::select(-createdAtMillis)

user_enrollment_status <- user_enrollment_status_uncleaned %>% mutate(
  time_last_earned_out = as.POSIXct(timestampOfLastEarnOut/1000, origin = "1970-01-01", tz = "UTC"), 
  time_last_state_change = as.POSIXct(timestampOfLastEarnOut/1000, origin = "1970-01-01", tz = "UTC")) %>% 
  dplyr::select(-timestampOfLastEarnOut, -timestampOfLastStateChange)

```


# Drop NA

There are two notes with missing text values - we'll drop them.
```{r}
notes <- notes %>% filter(!is.na(note_text))
```


Interesting to look at how the trends of notes have changed over time generally
- We can look at both how notes have changed over time, how ratings have changed over
time. The easiest way to do this is just to look at the counts per month. 

# Consider missing values 

How many missing values does a bad rating make? 

All the missing values come from the helpfulnessLevel column. There are 3,683
missing values.

```{r}
colSums(is.na(ratings)) # This gives us that only the helpfulnessLevel 
# value had missing values - 3693 of them

```

First it is helpful to group by month to see how this changes over time.

```{r}
notes <- notes %>% mutate(month_year = format(time_created, "%Y-%m")) 
ratings <- ratings %>% mutate(month_year = format(time_created, "%Y-%m")) 
```




```{r}
# Now let's count the missing values by their month
ratings %>% group_by(month_year) %>% 
  summarize(na_counts=sum(is.na(helpfulnessLevel)),
            non_na_counts=sum(!is.na(helpfulnessLevel)))
```


The output from this shows that the `helpfulnessLevel` metric is only missing 
values in the early months of 2021. Then, in June there are a few non-missing
values. This implies that this metric was introduced later, and for this 
reason there are no values in the early months of 2021. (Community Notes
was started in 2021). 


Now let's look at the notes missing values more generally. First, it is 
helpful to look at the number of missing values from each column. The code below
does this.

```{r}
colSums(is.na(notes))
```

From this, we found that the exact same number of pieces of missing data came
from three attributes: `harmful`, `believable`, `validationDifficulty`. There 
seems to be systematic discrepancy here and, again, looking at time will be 
helpful. We can just look at one of these three values in order to see the
number of missing values. Let's similarly count the number of missing values
for each `month_year` value to see if there is an identifiable trend. 


```{r}
notes %>% group_by(month_year) %>% 
  summarize(harmful_na=sum(is.na(harmful)),
            harmful_not_na=sum(!is.na(harmful)), 
            believable_na=sum(is.na(believable)),
            believable_not_na=sum(!is.na(believable)),
            validationDifficulty_na=sum(is.na(validationDifficulty)),
            validationDifficulty_not_na=sum(!is.na(validationDifficulty)))

print(notes %>% group_by(month_year) %>% 
  summarize(harmful_na=sum(is.na(harmful)),
            harmful_not_na=sum(!is.na(harmful)), 
            believable_na=sum(is.na(believable)),
            believable_not_na=sum(!is.na(believable)),
            validationDifficulty_na=sum(is.na(validationDifficulty)),
            validationDifficulty_not_na=sum(!is.na(validationDifficulty))), n=Inf, width=Inf)
```


The above output shows that though not exclusively missing, the NA values for `harmful`, `believable` and  `validationDifficulty` come from the same rows (the counts are the same for the na and non-na groups). 
There are no values 

All data points miss `harmful`, `believable` and  `validationDifficulty` values after November 2022 (this is also when the program went global). As a result, we drop the columns completely. 

```{r}
notes <- notes %>% select(-harmful, -believable, -validationDifficulty)
```


Now that we have month_year combinations we can look a bit into how the number of notes and ratings have changed over time. It is also interesting to see how many ratings per note there are. 

If we group by month and year, we see that the 

```{r}
notes %>% mutate(month_year = format(time_created, "%Y-%m")) %>% group_by(month_year) %>% summarize(notes=n())
```

```{r}
ggplot(data=notes, aes(x=month_year)) + geom_histogram(stat="count")
ggplot(data=ratings, aes(x=month_year)) + geom_histogram(stat="count")
ggplot(data=)


# Also interesting to look at ratings per note, and see how this has altered over time. 
```


From looking into this data, we think it makes sense to look at a time frame where the platform was being consistently used. Community Notes was initially launched only in the US, in January 2021 but was introduced globally on December 12th 2022. However, it didn't really pick up until March 2023 (the total count of notes went from 5000 to 15000 from February to April). 

Now let's look a bit into notes and ratings combinations. 


```{r}
# Create counts for how many ratings each note has 
ratings_note_counts <- ratings %>% group_by(noteId) %>% summarize(number_of_ratings=n())

# Add ratings count to the notes dataframe, this allows us to see how the notes compare 
# over time. 
notes_w_counts <- left_join(notes, ratings_note_counts, by="noteId")

```



# Beginnings of Exploring Harmful Notes - Look at how these measure with the Ukraine / Gaza 
# problems 


```{r}
# Look for notes that individuals have labeled as having the potential for considerable harm 
notes %>% filter(harmful == "CONSIDERABLE_HARM") %>% 

# Could consider looking at which topics are difficult to evaluate, but can cause considerable harm and how this interacts with the complexity of their history 
```


# Time Cleaning 

```{r}
colnames(notes)

```

# Look into how the specific topics, from topic modeling, match with different 
# categories for the complexity and such 

# Pull these directly from the open source code for community notes
# Also something to be said for how broad this is

```{python}
# This is the code in the X Community Notes topic_modeling.py file that declares specific topics 
Topics.UkraineConflict: {
    "ukrain",  # intentionally shortened for expanded matching
    "russia",
    "kiev",
    "kyiv",
    "moscow",
    "zelensky",
    "putin",
  },
  Topics.GazaConflict: {
    "israel",
    "palestin",  # intentionally shortened for expanded matching
    "gaza",
    "jerusalem",
    "\shamas\s",
  },
  Topics.MessiRonaldo: {
    "messi\s",  # intentional whitespace to prevent prefix matches
    "ronaldo",
  },
  Topics.Scams: {
    "scam",
    "undisclosed\sad",  # intentional whitespace
    "terms\sof\sservice",  # intentional whitespace
    "help\.x\.com",
    "x\.com/tos",
    "engagement\sfarm",  # intentional whitespace
    "spam",
    "gambling",
    "apostas",
    "apuestas",
    "dropship",
    "drop\sship",  # intentional whitespace
    "promotion",
  },
}

```


Limit to just English notes (we need to do this before we can do any work on the ratings)
 
Pre-trained language detection model 

```{r}
# fastText language detection 

# pretrained_fastText <- system.file("language_identification/lid.176.ftz", package = "fastText")
# 
# detect_english <- function(txt) {
#   language_output <- fastText::language_identification(
#     input_obj = txt,
#     pre_trained_language_model_path = pretrained_fastText,
#     k = 1,
#     th = 0.0,
#     verbose = FALSE)
#   
#   return(language_output$iso_lang_1 == "en")
#   
# }
# 
# problem_note <- batched_notes$`173`[172,"noteId"]
# notes <- notes %>% filter(!(noteId == problem_note))
# 
# batch_size <- 1000
# batched_notes <- notes %>% mutate(batch_id = (row_number() - 1) %/% batch_size) %>%
#   split(.$batch_id)
# 
# # 3. Apply a dplyr pipeline (e.g., summarize each batch)
# notes_list <- map(batched_notes, ~ .x %>% mutate(
#   text_length = length(note_text),
#   check_length = min(text_length, 100),
#   is_english = cldetect_english(substring(note_text, 1, check_length))))
# 
# 
# notes_lang_english <- bind_rows(notes_list)
# 
# # perform language detection and remove non-english notes (based on a threshold of 0.9)
# notes_h <- notes %>% slice_sample(n=10000) %>% mutate(
#   text_length = length(note_text), 
#   check_length = min(text_length, 100),
#   is_english = detect_english(substring(note_text, 1, check_length))) %>% 
#   filter(is_english)# %>% select(-is_english)
# 
# 
# 
```

# Langauge Detection - Limit to Only English
```{r}
# google version - the function returns NA if the language could not be
# reliably detected. 
notes_lang <- notes %>% mutate(
  is_english = 
    ifelse(cld2::detect_language(note_text) == "en", TRUE, FALSE))

notes_english <- notes_lang %>% filter(is_english)

```


Using ellmer docs: 
https://www.oneusefulthing.org/p/using-ai-right-now-a-quick-guide
https://ellmer.tidyverse.org/index.html
 







```{r}
OPENAI 

chat_openai("Be terse", model = "gpt-4o-mini")

```
 
 
 
 
 
 
 
